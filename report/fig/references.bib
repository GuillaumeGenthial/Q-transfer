@article{DBLP:journals/corr/ParisottoBS15,
  author    = {Emilio Parisotto and
               Lei Jimmy Ba and
               Ruslan Salakhutdinov},
  title     = {Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1511.06342},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06342},
  timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ParisottoBS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{DBLP:journals/corr/RusuCGDKPMKH15,
  author    = {Andrei A. Rusu and
               Sergio Gomez Colmenarejo and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Guillaume Desjardins and
               James Kirkpatrick and
               Razvan Pascanu and
               Volodymyr Mnih and
               Koray Kavukcuoglu and
               Raia Hadsell},
  title     = {Policy Distillation},
  journal   = {CoRR},
  volume    = {abs/1511.06295},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06295},
  timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/RusuCGDKPMKH15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/YosinskiCBL14,
  author    = {Jason Yosinski and
               Jeff Clune and
               Yoshua Bengio and
               Hod Lipson},
  title     = {How transferable are features in deep neural networks?},
  journal   = {CoRR},
  volume    = {abs/1411.1792},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.1792},
  timestamp = {Mon, 01 Dec 2014 14:32:13 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/YosinskiCBL14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@Article{JMLR09-taylor,
	Author="Matthew E.\ Taylor and Peter Stone",
	title="Transfer Learning for Reinforcement Learning Domains: A Survey",
        journal="Journal of Machine Learning Research",
	volume="10",number="1",
        pages="1633--1685",
	year="2009",
	abstract="The reinforcement learning paradigm is a popular way
        to address problems that have only limited environmental
        feedback, rather than correctly labeled examples, as is common
        in other machine learning contexts. While significant progress
        has been made to improve learning in a single task, the idea
        of transfer learning has only recently been applied to
        reinforcement learning tasks. The core idea of transfer is
        that experience gained in learning to perform one task can
        help improve learning performance in a related, but different,
        task. In this article we present a framework that classifies
        transfer learning methods in terms of their capabilities and
        goals, and then use it to survey the existing literature, as
        well as to suggest future directions for transfer learning
        work.",
	wwwnote={<a href="http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf">Official	version</a> from journal website.},
}

@Inbook{Lazaric2012,
author="Lazaric, Alessandro",
editor="Wiering, Marco
and van Otterlo, Martijn",
title="Transfer in Reinforcement Learning: A Framework and a Survey",
bookTitle="Reinforcement Learning: State-of-the-Art",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="143--173",
isbn="978-3-642-27645-3",
doi="10.1007/978-3-642-27645-3_5",
url="http://dx.doi.org/10.1007/978-3-642-27645-3_5"
}

@article{duinitial,
  title={Initial Progress in Transfer for Deep Reinforcement Learning Algorithms},
  author={Du, Yunshu and Gabriel, V and Irwin, James and Taylor, Matthew E}
}

@article{DBLP:journals/corr/RusuRDSKKPH16,
  author    = {Andrei A. Rusu and
               Neil C. Rabinowitz and
               Guillaume Desjardins and
               Hubert Soyer and
               James Kirkpatrick and
               Koray Kavukcuoglu and
               Razvan Pascanu and
               Raia Hadsell},
  title     = {Progressive Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1606.04671},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.04671},
  timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/RusuRDSKKPH16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Lazaric:2008:TSB:1390156.1390225,
 author = {Lazaric, Alessandro and Restelli, Marcello and Bonarini, Andrea},
 title = {Transfer of Samples in Batch Reinforcement Learning},
 booktitle = {Proceedings of the 25th International Conference on Machine Learning},
 series = {ICML '08},
 year = {2008},
 isbn = {978-1-60558-205-4},
 location = {Helsinki, Finland},
 pages = {544--551},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1390156.1390225},
 doi = {10.1145/1390156.1390225},
 acmid = {1390225},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@Article{JMLR07-taylor,
  author           = "Matthew E.\ Taylor and Peter Stone and Yaxin Liu",
  title            = "Transfer Learning via Inter-Task Mappings for Temporal Difference Learning",
  journal          = "Journal of Machine Learning Research",
  year             = "2007",
  volume           = "8",
  number           = "1",
  pages            = "2125--2167",
  abstract         = "Temporal difference (TD) learning has become a popular reinforcement learning technique in recent years. TD methods, relying on function approximators to generalize learning to novel situations, have had some experimental successes and have been shown to exhibit some desirable properties in theory, but the most basic algorithms have often been found slow in practice. This empirical result has motivated the development of many methods that speed up reinforcement learning by modifying a task for the learner or helping the learner better generalize to novel situations. This article focuses on generalizing across tasks, thereby speeding up learning, via a novel form of transfer using handcoded task relationships. We compare learning on a complex task with three function approximators, a cerebellar model arithmetic computer (CMAC), an artificial neural network (ANN), and a radial basis function (RBF), and empirically demonstrate that directly transferring the action-value function can lead to a dramatic speedup in learning with all three. Using transfer via inter-task mapping (tvitm), agents are able to learn one task and then markedly reduce the time it takes to learn a more complex task. Our algorithms are fully implemented and tested in the RoboCup soccer Keepaway domain.",
  wwwnote = {Available from <a href="http://jmlr.csail.mit.edu/papers/v8/taylor07a.html">journal's web page</a>.},
}

@article{DBLP:journals/corr/BorsaGS16,
  author    = {Diana Borsa and
               Thore Graepel and
               John Shawe{-}Taylor},
  title     = {Learning Shared Representations in Multi-task Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1603.02041},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.02041},
  timestamp = {Sat, 02 Apr 2016 11:49:48 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BorsaGS16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}
